{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import sys\n",
    "sys.path.append('/scratch/2023-fall-sp-le/langseg')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from  matplotlib import pyplot as plt\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from methods.prompt_engineering import extract_class_embeddings, extract_clip_text_embeddings\n",
    "BICUBIC = InterpolationMode.BICUBIC\n",
    "\n",
    "from datasets.coco_stuff import coco_stuff_categories\n",
    "from datasets.cityscapes import cat_to_label_id as cityscapes_cats\n",
    "from datasets.utils import get_dataset\n",
    "from utils.metrics import RunningScore\n",
    "from utils.plotting import *\n",
    "\n",
    "from methods.diffusion_patch import *\n",
    "from methods.diffusion import *\n",
    "from methods.diffusion_seg import *\n",
    "from methods.diffusion_utils import *\n",
    "from methods.text_embeddings import *\n",
    "from methods.diffseg import get_semantics, get_pred_mask\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class SegmentationConfig:\n",
    "#     dir_dataset: str = field(\n",
    "#         default=\"/sinergia/ozaydin/segment/STEGO-master/data/cocostuff\", metadata={\"help\": \"dir dataset\"}\n",
    "#     )\n",
    "#     dataset_name: str = field(\n",
    "#         default=\"voc2012\", metadata={\"help\": \"for get_dataset\"}\n",
    "#     )\n",
    "#     split: str = field(\n",
    "#         default=\"train\", metadata={\"help\": \"which split to use\"}\n",
    "#     )\n",
    "#     resolution: int = field(\n",
    "#         default=512, metadata={\"help\": \"resolution of the images, e.g, 512, 768, 1024\"}\n",
    "#     )\n",
    "#     mask_res: int = field(\n",
    "#         default=320, metadata={\"help\": \"resolution of the masks, e.g, 64, 320, 512\"}\n",
    "#     )\n",
    "#     dense_clip_arch: str = field(\n",
    "#         default=\"RN50x16\", metadata={\"help\": \"not used in cocostuff\"}\n",
    "#     )\n",
    "\n",
    "# args = SegmentationConfig()\n",
    "\n",
    "@dataclass\n",
    "class SegmentationConfig:\n",
    "    dir_dataset: str = field(\n",
    "        default=\"/scratch/2023-fall-sp-le/data/VOCdevkit/VOC2010\", metadata={\"help\": \"dir dataset\"}\n",
    "    )\n",
    "    dataset_name: str = field(\n",
    "        default=\"pascal_context\", metadata={\"help\": \"for get_dataset\"}\n",
    "    )\n",
    "    split: str = field(\n",
    "        default=\"val\", metadata={\"help\": \"which split to use\"}\n",
    "    )\n",
    "    resolution: int = field(\n",
    "        default=512, metadata={\"help\": \"resolution of the images, e.g, 512, 768, 1024\"}\n",
    "    )\n",
    "    mask_res: int = field(\n",
    "        default=320, metadata={\"help\": \"resolution of the masks, e.g, 64, 320, 512\"}\n",
    "    )\n",
    "    dense_clip_arch: str = field(\n",
    "        default=\"RN50x16\", metadata={\"help\": \"not used in cocostuff\"}\n",
    "    )\n",
    "\n",
    "args = SegmentationConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, categories, palette = get_dataset(\n",
    "    dir_dataset=args.dir_dataset,\n",
    "    dataset_name=args.dataset_name,\n",
    "    split=args.split,\n",
    "    resolution=args.resolution,\n",
    "    mask_res=args.mask_res,\n",
    "    dense_clip_arch=args.dense_clip_arch\n",
    ")\n",
    "\n",
    "if args.dataset_name == \"coco_stuff\":\n",
    "    label_id_to_cat = coco_stuff_categories\n",
    "    cat_to_label_id = {v: i for i, v in enumerate(label_id_to_cat)}\n",
    "elif args.dataset_name == \"cityscapes\":\n",
    "    cat_to_label_id = cityscapes_cats\n",
    "    label_id_to_cat = {i: c for c, i in cat_to_label_id.items()}\n",
    "elif args.dataset_name == \"voc2012\" or args.dataset_name == \"pascal_context\":\n",
    "    label_id_to_cat = categories\n",
    "    cat_to_label_id = {v: i for i, v in enumerate(label_id_to_cat)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxx = 4\n",
    "val_img = dataset[idxx][\"img\"].cpu().numpy()\n",
    "val_gt = dataset[idxx][\"gt\"].cpu().numpy()\n",
    "lab_ids = sorted(list(np.unique(val_gt)))\n",
    "lab_ids = np.array(lab_ids[1:])\n",
    "val_pil_img = render_results(val_img, val_gt, palette)\n",
    "_ = get_legends(lab_ids, palette, label_id_to_cat, is_voc2012 = args.dataset_name == \"voc2012\")\n",
    "\n",
    "val_gt = dataset[idxx][\"gt\"].unsqueeze(0)\n",
    "val_labels = sorted(np.unique(val_gt))\n",
    "val_labels = [l for l in val_labels if l != -1] # don't process label -1 (ignored unlabelled pixels)\n",
    "val_labels = [label_id_to_cat[c] for c in val_labels]\n",
    "val_labels = [l for l in val_labels if l != \"background\"] # don't feed \"background\" as text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from methods import gem\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# model_name = 'ViT-B/16'  # 'ViT-B-16-quickgelu'\n",
    "# pretrained = 'openai'  # 'metaclip_400m'\n",
    "model_name = 'ViT-B/16-quickgelu'\n",
    "# model_name = 'ViT-L/14-quickgelu'\n",
    "pretrained = 'metaclip_400m'\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# init model and image transform\n",
    "gem_model = gem.create_gem_model(model_name=model_name,\n",
    "                                 pretrained=pretrained, \n",
    "                                 device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image and text\n",
    "preprocess = gem.get_gem_img_transform(img_size=(448, 448))\n",
    "image = preprocess(Image.open(dataset[idxx][\"p_img\"])).unsqueeze(0).to(device)\n",
    "class_names = label_id_to_cat#[1:]\n",
    "with torch.no_grad():\n",
    "    logits = gem_model(image, class_names, normalize=False, return_ori=False)  # [1, num_class, W, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_logits = gem_model.min_max(logits)\n",
    "gem.visualize(image, class_names, normed_logits)  # (optional visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gem_voc():\n",
    "    from tqdm import tqdm\n",
    "    preprocess = gem.get_gem_img_transform(img_size=(448, 448))\n",
    "    running_score = RunningScore(len(label_id_to_cat))\n",
    "    pbar = tqdm(range(len(dataset)))\n",
    "    # qs = []\n",
    "    for idx in pbar:\n",
    "        val_img = dataset[idx][\"img\"].permute(1,2,0)[None,...].numpy()\n",
    "        val_gt = dataset[idx][\"gt\"].unsqueeze(0)\n",
    "        val_labels = sorted(np.unique(val_gt))\n",
    "        val_labels = [l for l in val_labels if l != -1] # don't process label -1 (ignored unlabelled pixels)\n",
    "        val_labels = [label_id_to_cat[c] for c in val_labels]\n",
    "        val_labels = [l for l in val_labels if l != \"background\"] # don't feed \"background\" as text input\n",
    "        # if len(val_labels) == 1: continue\n",
    "\n",
    "        # load image and text\n",
    "        image = preprocess(Image.open(dataset[idx][\"p_img\"])).unsqueeze(0).to(device)\n",
    "        class_names = label_id_to_cat#[1:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = gem_model(image, class_names, output_size=320, normalize=False)  # [1, num_class, W, H]\n",
    "\n",
    "        pred = logits.argmax(dim=1)\n",
    "        \n",
    "        # probs = (logits * 10).softmax(dim=1) # without softmax -> person class VERY bad\n",
    "        # max_probs = probs.max(dim=1)[0]\n",
    "        # bg_probs = 1 - max_probs - 0.8\n",
    "        # bg_probs = torch.where(bg_probs > 0, bg_probs, 0)\n",
    "        # # bg_probs = probs.max() - probs.max(dim=1)[0]\n",
    "        # pred = torch.cat([bg_probs[None], probs], dim=1)\n",
    "        # pred = pred.argmax(dim=1)\n",
    "\n",
    "        # probs = (logits * 100).softmax(dim=1)\n",
    "        # ppls = (-probs * probs.log()).sum(dim=1).exp() # perplexity, aka effective no. outcomes\n",
    "        # # median = entropies.quantile(0.5).item()\n",
    "        # # mean = entropies.mean().item()\n",
    "        # # thresh = 1.5 * median/mean\n",
    "        # # thresh = np.maximum(thresh, 1.1)\n",
    "        # thresh = 1.5\n",
    "        # bg_mask = ppls > thresh\n",
    "        # # bg_mask = val_gt == 0\n",
    "        # pred = logits.argmax(dim=1) + 1\n",
    "        # pred[bg_mask] = 0\n",
    "\n",
    "        # concept_indices = [[i] for i in range(21)]\n",
    "        # nouns = label_id_to_cat\n",
    "        # probs = (logits*10).softmax(dim=1)\n",
    "        # bg_probs = torch.full_like(probs[:, 0], -100)\n",
    "        # pred = torch.cat([bg_probs[None], probs], dim=1)\n",
    "        # x_weight = pred.permute(0, 2, 3, 1).squeeze(0)\n",
    "        # pred_mask = val_gt.squeeze(0)\n",
    "        # label_to_mask = get_semantics(pred_mask, x_weight, concept_indices, nouns, voting=\"mean\", background=False)\n",
    "        # pred_mask = get_pred_mask(pred_mask, label_to_mask, cat_to_label_id)\n",
    "        # pred = torch.from_numpy(pred_mask[None])\n",
    "\n",
    "        # concept_indices = [[i] for i in range(20)]\n",
    "        # nouns = label_id_to_cat[1:]\n",
    "        # probs = (logits*100).softmax(dim=1)\n",
    "        # x_weight = probs.permute(0, 2, 3, 1).squeeze(0)\n",
    "        # pred_mask = val_gt.squeeze(0)\n",
    "        # label_to_mask = get_semantics(pred_mask, x_weight, concept_indices, nouns, voting=\"mean\", background=False)\n",
    "        # pred_mask = get_pred_mask(pred_mask, label_to_mask, cat_to_label_id)\n",
    "        # pred = torch.from_numpy(pred_mask)[None]\n",
    "        # pred[val_gt == 0] = 0\n",
    "        \n",
    "        # probs = (logits * 100).softmax(dim=1)\n",
    "        # pred = probs.argmax(dim=1) + 1\n",
    "        # max_prob = probs.max(dim=1)[0]\n",
    "        # bg_ratio = 0.9 #1-logits.softmax(dim=1).max()/probs.max()\n",
    "        # bg_thresh = bg_ratio*probs.max()\n",
    "        # pred[max_prob < bg_thresh] = 0\n",
    "\n",
    "        # eliminating false positives\n",
    "        # pred_classes = torch.unique(pred)\n",
    "        # for i, c in enumerate(label_id_to_cat):\n",
    "        #     if i in pred_classes and c not in [\"background\"]+val_labels:\n",
    "        #         pred[pred == i] = 0\n",
    "\n",
    "        running_score.update(val_gt.cpu().numpy(), pred.cpu().numpy())\n",
    "        metrics, cls_iou = running_score.get_scores()\n",
    "        miou = metrics[\"Mean IoU\"]\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"mIoU {miou:.3f}\"\n",
    "        )\n",
    "    return running_score\n",
    "\n",
    "running_score = run_gem_voc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, cls_iou = running_score.get_scores() # original results\n",
    "print({k: f\"{v*100:.1f}\" for k, v in metrics.items()})\n",
    "print({label_id_to_cat[i]: f\"{v*100:.1f}\" for i, v in cls_iou.items()})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
