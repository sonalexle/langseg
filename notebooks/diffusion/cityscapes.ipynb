{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from PIL import Image\n",
    "import os, math, warnings\n",
    "import sys\n",
    "sys.path.append('/scratch/2023-fall-sp-le/langseg')\n",
    "\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets.coco_stuff import coco_stuff_categories\n",
    "from datasets.cityscapes import cat_to_label_id as cityscapes_cats\n",
    "from datasets.utils import get_dataset\n",
    "from utils.metrics import RunningScore\n",
    "from methods.diffusion_patch import *\n",
    "from methods.diffusion import *\n",
    "from methods.diffusion_utils import *\n",
    "from methods.text_embeddings import *\n",
    "from methods.diffusion_seg import *\n",
    "from methods.grabcut import *\n",
    "from methods.pipeline_patch import patch_sd_call, patch_sdxl_call, patch_sd_prepare_latents\n",
    "from methods.multilabel_classifiers import CLIPMultilabelClassifier, BLIPMultilabelClassifier\n",
    "#pip install --upgrade diffusers transformers nltk accelerate torch_kmeans igraph peft compel torchvision ftfy open_clip_torch einops\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SegmentationConfig:\n",
    "    dir_dataset: str = field(\n",
    "        default=\"/sinergia/ozaydin/segment/STEGO-master/data/cityscapes\", metadata={\"help\": \"dir dataset\"}\n",
    "    )\n",
    "    dataset_name: str = field(\n",
    "        default=\"cityscapes\", metadata={\"help\": \"for get_dataset\"}\n",
    "    )\n",
    "    split: str = field(\n",
    "        default=\"val\", metadata={\"help\": \"which split to use\"}\n",
    "    )\n",
    "    resolution: int = field(\n",
    "        default=512, metadata={\"help\": \"resolution of the images, e.g, 512, 768, 1024\"}\n",
    "    )\n",
    "    mask_res: int = field(\n",
    "        default=320, metadata={\"help\": \"resolution of the masks, e.g, 64, 320, 512\"}\n",
    "    )\n",
    "    dense_clip_arch: str = field(\n",
    "        default=\"RN50x16\", metadata={\"help\": \"not used in cocostuff\"}\n",
    "    )\n",
    "\n",
    "args = SegmentationConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, categories, palette = get_dataset(\n",
    "    dir_dataset=args.dir_dataset,\n",
    "    dataset_name=args.dataset_name,\n",
    "    split=args.split,\n",
    "    resolution=args.resolution,\n",
    "    mask_res=args.mask_res,\n",
    "    dense_clip_arch=args.dense_clip_arch\n",
    ")\n",
    "label_id_to_cat = categories\n",
    "cat_to_label_id = {v: i for i, v in enumerate(label_id_to_cat)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285214d25f8548f1aca54108cb09a3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "# model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "# model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16,\n",
    "    use_safetensors=True, variant=\"fp16\",\n",
    "    # device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# unet_id = \"mhdang/dpo-sd1.5-text2image-v1\"\n",
    "# unet = UNet2DConditionModel.from_pretrained(unet_id, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "# pipe.unet = unet\n",
    "\n",
    "# load_model_weights(pipe, './TexForce/lora_weights/sd15_refl/', 'unet+lora')\n",
    "# load_model_weights(pipe, './TexForce/lora_weights/sd15_texforce/', 'text+lora')\n",
    "# pipe.forward = patch_sd_call(pipe)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "# pipe.enable_freeu(s1=0.9, s2=0.2, b1=1.4, b2=1.6)\n",
    "configure_ldm(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = {}\n",
    "attention_layers_to_use = ATTENTION_LAYERS\n",
    "attention_store = AttentionStore(low_resource=False, no_uncond=True, layer_keys=attention_layers_to_use)\n",
    "attention_store.num_att_layers = len(attention_layers_to_use)\n",
    "handles = register_attention_hooks(pipe.unet, attention_store, attention_layers_to_use, handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]/scratch/2023-fall-sp-le/langseg/utils/metrics.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
      "/scratch/2023-fall-sp-le/langseg/utils/metrics.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
      "mIoU_0 0.112 | : 100%|██████████| 500/500 [03:04<00:00,  2.72it/s]\n"
     ]
    }
   ],
   "source": [
    "def run_exp():\n",
    "    running_score_0 = RunningScore(len(coco_stuff_categories))\n",
    "    running_score_1 = RunningScore(len(coco_stuff_categories))\n",
    "    running_score_2 = RunningScore(len(coco_stuff_categories))\n",
    "    remapping = torch.tensor(list(coco_stuff_171_to_27.keys())).cuda(), torch.tensor(list(coco_stuff_171_to_27.values())).cuda()\n",
    "    pbar = tqdm(range(len(dataset)))\n",
    "    for idx in pbar:\n",
    "        val_img = dataset[idx][\"img\"].permute(1,2,0)[None,...].numpy()\n",
    "        val_gt = dataset[idx][\"gt\"].unsqueeze(0)\n",
    "        val_labels = sorted(np.unique(val_gt))\n",
    "        val_labels = [l for l in val_labels if l != -1] # don't process label -1 (ignored unlabelled pixels)\n",
    "        val_labels = [label_id_to_cat[c] for c in val_labels]\n",
    "        val_labels = [l for l in val_labels if l != \"background\"] # don't feed \"background\" as text input\n",
    "\n",
    "        # image, y_true = get_image_and_labels(idx)\n",
    "        # y_pred, _ = clip_classifier(image, choice=(8,8), clf_thresh=0.5)\n",
    "        # y_pred = y_pred.cpu().numpy()\n",
    "        # try:\n",
    "        #     val_labels = get_pred_label_names(y_pred, label_id_to_cat[1:])\n",
    "        # except AssertionError:\n",
    "        #     image_indices_failed_clf.append(idx)\n",
    "        #     continue\n",
    "\n",
    "        # text_embeds, concept_ind, _ = get_text_embeddings(pipe.tokenizer, pipe.text_encoder, val_labels, label_id_to_cat)\n",
    "        # text_embeds, concept_ind, _ = get_contextualized_text_embeddings(pipe.tokenizer, pipe.text_encoder, val_labels)\n",
    "        text_embeds, concept_ind, concept_indices, _ = get_txt_embeddings(\n",
    "            pipe.tokenizer, pipe.text_encoder, val_labels, label_id_to_cat, cat_to_label_id,\n",
    "            use_compel=False\n",
    "        )\n",
    "\n",
    "        val_img = val_img.repeat(len(text_embeds), axis=0)\n",
    "        attention_store.reset()\n",
    "\n",
    "        # latents = image2latent(pipe.vae, val_img, normalize=False)\n",
    "        # latents, _, _ = get_noisy_latents(pipe.scheduler, latents)\n",
    "        # pipe.forward(prompt_embeds=text_embeds, latents=latents, guidance_scale=0)\n",
    "\n",
    "        training_step(pipe, text_embeds, val_img, attention_store, no_uncond=True, normalize=False, low_resource=False)\n",
    "\n",
    "        ca, sa = get_attention_maps(\n",
    "            attention_store.get_average_attention(),\n",
    "            batch_size=1,\n",
    "            label_indices=concept_indices,\n",
    "            output_size=64,\n",
    "            average_layers=True,\n",
    "            apply_softmax=True,\n",
    "            softmax_dim=-1,\n",
    "            simple_average=False\n",
    "        )\n",
    "        agg_map = get_agg_map(ca, sa, walk_len=1, beta=1, minmax_norm=False)\n",
    "\n",
    "        pred0 = get_random_walk_mask(\n",
    "            agg_map, cat_to_label_id,\n",
    "            concept_ind, val_labels, args.mask_res\n",
    "        ).long()\n",
    "        \n",
    "        pred1 = get_specclust_mask(\n",
    "            agg_map, sa, cat_to_label_id,\n",
    "            concept_ind, val_labels,\n",
    "            output_size=args.mask_res\n",
    "        ).long()\n",
    "\n",
    "        pred2 = diffseg(sa, out_res=args.mask_res, refine=True, kl_thresh=0.8)\n",
    "        agg_map = upscale_attn(agg_map, 320, is_cross=True)[..., concept_ind]\n",
    "        pred2 = label_clusters(pred2, agg_map, reshape_logits=False)\n",
    "        for c in pred2.unique().tolist():        \n",
    "            pred2[pred2 == c] = cat_to_label_id[val_labels[c]]\n",
    "\n",
    "        running_score_0.update(val_gt.cpu().numpy(), pred0.cpu().numpy())\n",
    "        metrics_0, cls_iou_0 = running_score_0.get_scores()\n",
    "        miou_0 = metrics_0[\"Mean IoU\"]\n",
    "\n",
    "        running_score_1.update(val_gt.cpu().numpy(), pred1.cpu().numpy())\n",
    "        metrics_1, cls_iou_1 = running_score_1.get_scores()\n",
    "        miou_1 = metrics_1[\"Mean IoU\"]\n",
    "\n",
    "        running_score_2.update(val_gt.cpu().numpy(), pred2.cpu().numpy())\n",
    "        metrics_2, cls_iou_2 = running_score_2.get_scores()\n",
    "        miou_2 = metrics_2[\"Mean IoU\"]\n",
    "\n",
    "\n",
    "    #     pred2 = DiffusionGraphCut(\n",
    "    #         agg_map.cpu(),\n",
    "    #         sa.cpu(),\n",
    "    #         concept_ind, val_labels, cat_to_label_id\n",
    "    #     )(args.mask_res)[None]\n",
    "    #     pred2 = pred2.cpu().numpy()\n",
    "    #     running_score_2.update(val_gt.cpu().numpy(), pred2)\n",
    "    #     metrics_2, cls_iou_2 = running_score_2.get_scores()\n",
    "    #     miou_2 = metrics_2[\"Mean IoU\"]\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"mIoU_0 {miou_0:.3f} | \"\n",
    "            f\"mIoU_1 {miou_1:.3f} |\"\n",
    "            f\"mIoU_2 {miou_2:.3f}\"\n",
    "        )\n",
    "    return running_score_0, running_score_1, running_score_2\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    rs = run_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pixel Acc': '31.7', 'Mean Acc': '41.9', 'FreqW Acc': '24.8', 'Mean IoU': '11.2'}\n",
      "{'road': '9.1', 'sidewalk': '10.6', 'parking lot': '2.3', 'rail track': '15.3', 'building': '35.0', 'wall': '6.2', 'fence': '11.6', 'guard rail': '4.8', 'bridge': '7.7', 'tunnel': 'nan', 'pole': '5.0', 'polegroup': '0.0', 'traffic light': '0.6', 'traffic sign': '1.5', 'vegetation': '55.4', 'terrain': '0.6', 'sky': '19.3', 'person': '5.1', 'rider': '0.2', 'car': '39.7', 'truck': '17.6', 'bus': '15.3', 'caravan': '2.2', 'trailer': '8.4', 'train': '11.6', 'motorcycle': '0.7', 'bicycle': '6.6'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rs)):\n",
    "    metrics, cls_iou = rs[i].get_scores() # original results\n",
    "    print({k: f\"{v*100:.1f}\" for k, v in metrics.items()})\n",
    "    print({label_id_to_cat[i]: f\"{v*100:.1f}\" for i, v in cls_iou.items()})\n",
    "    print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val results\n",
    "\n",
    "{'Pixel Acc': '31.7', 'Mean Acc': '41.9', 'FreqW Acc': '24.8', 'Mean IoU': '11.2'}\n",
    "\n",
    "{'road': '9.1', 'sidewalk': '10.6', 'parking lot': '2.3', 'rail track': '15.3', 'building': '35.0', 'wall': '6.2', 'fence': '11.6', 'guard rail': '4.8', 'bridge': '7.7', 'tunnel': 'nan', 'pole': '5.0', 'polegroup': '0.0', 'traffic light': '0.6', 'traffic sign': '1.5', 'vegetation': '55.4', 'terrain': '0.6', 'sky': '19.3', 'person': '5.1', 'rider': '0.2', 'car': '39.7', 'truck': '17.6', 'bus': '15.3', 'caravan': '2.2', 'trailer': '8.4', 'train': '11.6', 'motorcycle': '0.7', 'bicycle': '6.6'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
