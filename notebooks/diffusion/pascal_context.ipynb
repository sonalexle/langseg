{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from PIL import Image\n",
    "import os, math, warnings\n",
    "import sys\n",
    "sys.path.append('/scratch/2023-fall-sp-le/langseg')\n",
    "\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets.coco_stuff import coco_stuff_categories\n",
    "from datasets.cityscapes import cat_to_label_id as cityscapes_cats\n",
    "from datasets.utils import get_dataset\n",
    "from utils.metrics import RunningScore\n",
    "from methods.diffusion_patch import *\n",
    "from methods.diffusion import *\n",
    "from methods.diffusion_utils import *\n",
    "from methods.text_embeddings import *\n",
    "from methods.diffusion_seg import *\n",
    "from methods.grabcut import *\n",
    "from methods.pipeline_patch import patch_sd_call, patch_sdxl_call, patch_sd_prepare_latents\n",
    "from methods.multilabel_classifiers import CLIPMultilabelClassifier, BLIPMultilabelClassifier\n",
    "#pip install --upgrade diffusers transformers nltk accelerate torch_kmeans igraph peft compel torchvision ftfy open_clip_torch einops\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SegmentationConfig:\n",
    "    dir_dataset: str = field(\n",
    "        default=\"/scratch/2023-fall-sp-le/data/VOCdevkit/VOC2010\", metadata={\"help\": \"dir dataset\"}\n",
    "    )\n",
    "    dataset_name: str = field(\n",
    "        default=\"pascal_context\", metadata={\"help\": \"for get_dataset\"}\n",
    "    )\n",
    "    split: str = field(\n",
    "        default=\"train\", metadata={\"help\": \"which split to use\"}\n",
    "    )\n",
    "    resolution: int = field(\n",
    "        default=512, metadata={\"help\": \"resolution of the images, e.g, 512, 768, 1024\"}\n",
    "    )\n",
    "    mask_res: int = field(\n",
    "        default=320, metadata={\"help\": \"resolution of the masks, e.g, 64, 320, 512\"}\n",
    "    )\n",
    "    dense_clip_arch: str = field(\n",
    "        default=\"RN50x16\", metadata={\"help\": \"not used in cocostuff\"}\n",
    "    )\n",
    "\n",
    "args = SegmentationConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, categories, palette = get_dataset(\n",
    "    dir_dataset=args.dir_dataset,\n",
    "    dataset_name=args.dataset_name,\n",
    "    split=args.split,\n",
    "    resolution=args.resolution,\n",
    "    mask_res=args.mask_res,\n",
    "    dense_clip_arch=args.dense_clip_arch\n",
    ")\n",
    "label_id_to_cat = categories\n",
    "cat_to_label_id = {v: i for i, v in enumerate(label_id_to_cat)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572c4f5365d045cf8980ff4fd639ec11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "# model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "# model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16,\n",
    "    use_safetensors=True, variant=\"fp16\",\n",
    "    # device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# unet_id = \"mhdang/dpo-sd1.5-text2image-v1\"\n",
    "# unet = UNet2DConditionModel.from_pretrained(unet_id, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "# pipe.unet = unet\n",
    "\n",
    "# load_model_weights(pipe, './TexForce/lora_weights/sd15_refl/', 'unet+lora')\n",
    "# load_model_weights(pipe, './TexForce/lora_weights/sd15_texforce/', 'text+lora')\n",
    "# pipe.forward = patch_sd_call(pipe)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "# pipe.enable_freeu(s1=0.9, s2=0.2, b1=1.4, b2=1.6)\n",
    "configure_ldm(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = {}\n",
    "attention_layers_to_use = ATTENTION_LAYERS\n",
    "attention_store = AttentionStore(low_resource=False, no_uncond=True, layer_keys=attention_layers_to_use)\n",
    "attention_store.num_att_layers = len(attention_layers_to_use)\n",
    "handles = register_attention_hooks(pipe.unet, attention_store, attention_layers_to_use, handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mIoU_0 0.438 | : 100%|██████████| 4996/4996 [26:39<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def run_exp():\n",
    "    running_score_0 = RunningScore(len(label_id_to_cat))\n",
    "    running_score_1 = RunningScore(len(label_id_to_cat))\n",
    "    running_score_2 = RunningScore(len(label_id_to_cat))\n",
    "\n",
    "    pbar = tqdm(range(len(dataset)))\n",
    "    for idx in pbar:\n",
    "        val_img = dataset[idx][\"img\"].permute(1,2,0)[None,...].numpy()\n",
    "        val_gt = dataset[idx][\"gt\"].unsqueeze(0)\n",
    "        val_labels = sorted(np.unique(val_gt))\n",
    "        val_labels = [l for l in val_labels if l != -1] # don't process label -1 (ignored unlabelled pixels)\n",
    "        val_labels = [label_id_to_cat[c] for c in val_labels]\n",
    "        val_labels = [l for l in val_labels if l != \"background\"] # don't feed \"background\" as text input\n",
    "\n",
    "        # image, y_true = get_image_and_labels(idx)\n",
    "        # y_pred, _ = clip_classifier(image, choice=(8,8), clf_thresh=0.5)\n",
    "        # y_pred = y_pred.cpu().numpy()\n",
    "        # try:\n",
    "        #     val_labels = get_pred_label_names(y_pred, label_id_to_cat[1:])\n",
    "        # except AssertionError:\n",
    "        #     image_indices_failed_clf.append(idx)\n",
    "        #     continue\n",
    "\n",
    "        # text_embeds, concept_ind, _ = get_text_embeddings(pipe.tokenizer, pipe.text_encoder, val_labels, label_id_to_cat)\n",
    "        # text_embeds, concept_ind, _ = get_contextualized_text_embeddings(pipe.tokenizer, pipe.text_encoder, val_labels)\n",
    "        text_embeds, concept_ind, concept_indices, _ = get_txt_embeddings(\n",
    "            pipe.tokenizer, pipe.text_encoder, val_labels, label_id_to_cat, cat_to_label_id,\n",
    "            use_compel=False\n",
    "        )\n",
    "\n",
    "        val_img = val_img.repeat(len(text_embeds), axis=0)\n",
    "        attention_store.reset()\n",
    "\n",
    "        # latents = image2latent(pipe.vae, val_img, normalize=False)\n",
    "        # latents, _, _ = get_noisy_latents(pipe.scheduler, latents)\n",
    "        # pipe.forward(prompt_embeds=text_embeds, latents=latents, guidance_scale=0)\n",
    "\n",
    "        training_step(pipe, text_embeds, val_img, attention_store, no_uncond=True, normalize=False, low_resource=False)\n",
    "\n",
    "        ca, sa = get_attention_maps(\n",
    "            attention_store.get_average_attention(),\n",
    "            batch_size=1,\n",
    "            label_indices=concept_indices,\n",
    "            output_size=64,\n",
    "            average_layers=True,\n",
    "            apply_softmax=True,\n",
    "            softmax_dim=-1,\n",
    "            simple_average=False\n",
    "        )\n",
    "        agg_map = get_agg_map(ca, sa, walk_len=1, beta=1, minmax_norm=False)\n",
    "\n",
    "        pred0 = get_random_walk_mask(\n",
    "            agg_map, cat_to_label_id,\n",
    "            concept_ind, val_labels, args.mask_res\n",
    "        ).long()\n",
    "        \n",
    "#         pred1 = get_specclust_mask(\n",
    "#             agg_map, sa, cat_to_label_id,\n",
    "#             concept_ind, val_labels,\n",
    "#             output_size=args.mask_res\n",
    "#         ).long()\n",
    "\n",
    "#         pred2 = diffseg(sa, out_res=args.mask_res, refine=True, kl_thresh=0.8)\n",
    "#         agg_map = upscale_attn(agg_map, 320, is_cross=True)[..., concept_ind]\n",
    "#         pred2 = label_clusters(pred2, agg_map, reshape_logits=False)\n",
    "#         for c in pred2.unique().tolist():        \n",
    "#             pred2[pred2 == c] = cat_to_label_id[val_labels[c]]\n",
    "\n",
    "        running_score_0.update(val_gt.cpu().numpy(), pred0.cpu().numpy())\n",
    "        metrics_0, cls_iou_0 = running_score_0.get_scores()\n",
    "        miou_0 = metrics_0[\"Mean IoU\"]\n",
    "\n",
    "#         running_score_1.update(val_gt.cpu().numpy(), pred1.cpu().numpy())\n",
    "#         metrics_1, cls_iou_1 = running_score_1.get_scores()\n",
    "#         miou_1 = metrics_1[\"Mean IoU\"]\n",
    "\n",
    "#         running_score_2.update(val_gt.cpu().numpy(), pred2.cpu().numpy())\n",
    "#         metrics_2, cls_iou_2 = running_score_2.get_scores()\n",
    "#         miou_2 = metrics_2[\"Mean IoU\"]\n",
    "\n",
    "\n",
    "    #     pred2 = DiffusionGraphCut(\n",
    "    #         agg_map.cpu(),\n",
    "    #         sa.cpu(),\n",
    "    #         concept_ind, val_labels, cat_to_label_id\n",
    "    #     )(args.mask_res)[None]\n",
    "    #     pred2 = pred2.cpu().numpy()\n",
    "    #     running_score_2.update(val_gt.cpu().numpy(), pred2)\n",
    "    #     metrics_2, cls_iou_2 = running_score_2.get_scores()\n",
    "    #     miou_2 = metrics_2[\"Mean IoU\"]\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"mIoU_0 {miou_0:.3f} | \"\n",
    "            # f\"mIoU_1 {miou_1:.3f} |\"\n",
    "            # f\"mIoU_2 {miou_2:.3f}\"\n",
    "        )\n",
    "    return [running_score_0]#, running_score_1, running_score_2]\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    rs = run_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pixel Acc': '67.4', 'Mean Acc': '67.0', 'FreqW Acc': '52.5', 'Mean IoU': '43.8'}\n",
      "{'aeroplane': '54.1', 'bag': '12.6', 'bed': '54.5', 'bedclothes': '57.3', 'bench': '24.8', 'bicycle': '56.0', 'bird': '61.2', 'boat': '49.6', 'book': '14.8', 'bottle': '52.6', 'building': '39.3', 'bus': '74.7', 'cabinet': '41.2', 'car': '67.4', 'cat': '81.4', 'ceiling': '41.7', 'chair': '49.2', 'cloth': '32.4', 'computer': '14.8', 'cow': '74.3', 'cup': '21.3', 'curtain': '45.3', 'dog': '76.0', 'door': '37.1', 'fence': '35.7', 'floor': '61.8', 'flower': '33.9', 'food': '25.1', 'grass': '48.4', 'ground': '43.4', 'horse': '70.4', 'keyboard': '38.8', 'light': '5.7', 'motorbike': '68.5', 'mountain': '41.3', 'mouse': '7.0', 'person': '49.4', 'plate': '21.3', 'platform': '20.4', 'pottedplant': '51.6', 'road': '53.3', 'rock': '30.1', 'sheep': '65.5', 'shelves': '25.9', 'sidewalk': '23.6', 'sign': '9.8', 'sky': '51.5', 'snow': '47.6', 'sofa': '62.1', 'table': '35.3', 'track': '32.2', 'train': '64.8', 'tree': '52.4', 'truck': '26.4', 'tvmonitor': '52.3', 'wall': '53.1', 'water': '61.6', 'window': '40.8', 'wood': '38.1'}\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rs)):\n",
    "    metrics, cls_iou = rs[i].get_scores() # original results\n",
    "    print({k: f\"{v*100:.1f}\" for k, v in metrics.items()})\n",
    "    print({label_id_to_cat[i]: f\"{v*100:.1f}\" for i, v in cls_iou.items()})\n",
    "    print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train results\n",
    "\n",
    "{'Pixel Acc': '67.4', 'Mean Acc': '67.0', 'FreqW Acc': '52.5', 'Mean IoU': '43.8'}\n",
    "\n",
    "{'aeroplane': '54.1', 'bag': '12.6', 'bed': '54.5', 'bedclothes': '57.3', 'bench': '24.8', 'bicycle': '56.0', 'bird': '61.2', 'boat': '49.6', 'book': '14.8', 'bottle': '52.6', 'building': '39.3', 'bus': '74.7', 'cabinet': '41.2', 'car': '67.4', 'cat': '81.4', 'ceiling': '41.7', 'chair': '49.2', 'cloth': '32.4', 'computer': '14.8', 'cow': '74.3', 'cup': '21.3', 'curtain': '45.3', 'dog': '76.0', 'door': '37.1', 'fence': '35.7', 'floor': '61.8', 'flower': '33.9', 'food': '25.1', 'grass': '48.4', 'ground': '43.4', 'horse': '70.4', 'keyboard': '38.8', 'light': '5.7', 'motorbike': '68.5', 'mountain': '41.3', 'mouse': '7.0', 'person': '49.4', 'plate': '21.3', 'platform': '20.4', 'pottedplant': '51.6', 'road': '53.3', 'rock': '30.1', 'sheep': '65.5', 'shelves': '25.9', 'sidewalk': '23.6', 'sign': '9.8', 'sky': '51.5', 'snow': '47.6', 'sofa': '62.1', 'table': '35.3', 'track': '32.2', 'train': '64.8', 'tree': '52.4', 'truck': '26.4', 'tvmonitor': '52.3', 'wall': '53.1', 'water': '61.6', 'window': '40.8', 'wood': '38.1'}\n",
    "\n",
    "### val results\n",
    "\n",
    "{'Pixel Acc': '67.8', 'Mean Acc': '69.2', 'FreqW Acc': '52.9', 'Mean IoU': '45.1'}\n",
    "\n",
    "{'aeroplane': '56.6', 'bag': '14.3', 'bed': '54.8', 'bedclothes': '59.3', 'bench': '39.4', 'bicycle': '56.8', 'bird': '55.3', 'boat': '51.2', 'book': '17.7', 'bottle': '60.3', 'building': '39.6', 'bus': '76.9', 'cabinet': '46.6', 'car': '70.7', 'cat': '81.5', 'ceiling': '45.5', 'chair': '49.0', 'cloth': '30.6', 'computer': '21.9', 'cow': '71.8', 'cup': '18.5', 'curtain': '44.2', 'dog': '75.0', 'door': '35.6', 'fence': '38.4', 'floor': '61.9', 'flower': '32.6', 'food': '29.8', 'grass': '48.9', 'ground': '43.0', 'horse': '73.4', 'keyboard': '46.9', 'light': '6.8', 'motorbike': '68.3', 'mountain': '36.3', 'mouse': '6.1', 'person': '49.1', 'plate': '21.3', 'platform': '21.7', 'pottedplant': '52.1', 'road': '52.3', 'rock': '38.9', 'sheep': '66.6', 'shelves': '29.8', 'sidewalk': '25.3', 'sign': '14.2', 'sky': '51.5', 'snow': '52.2', 'sofa': '55.6', 'table': '41.7', 'track': '31.4', 'train': '64.6', 'tree': '53.8', 'truck': '26.6', 'tvmonitor': '54.8', 'wall': '53.3', 'water': '61.7', 'window': '42.3', 'wood': '35.5'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
