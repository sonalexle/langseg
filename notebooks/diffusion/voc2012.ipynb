{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from PIL import Image\n",
    "import os, math, warnings\n",
    "import sys\n",
    "sys.path.append('/scratch/2023-fall-sp-le/langseg')\n",
    "\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets.coco_stuff import coco_stuff_categories\n",
    "from datasets.cityscapes import cat_to_label_id as cityscapes_cats\n",
    "from datasets.utils import get_dataset\n",
    "from utils.metrics import RunningScore\n",
    "from methods.diffusion_patch import *\n",
    "from methods.diffusion import *\n",
    "from methods.diffusion_utils import *\n",
    "from methods.text_embeddings import *\n",
    "from methods.diffusion_seg import *\n",
    "from methods.exp_utils import *\n",
    "from methods.grabcut import *\n",
    "from methods.pipeline_patch import patch_sd_call, patch_sdxl_call, patch_sd_prepare_latents\n",
    "from methods.multilabel_classifiers import CLIPMultilabelClassifier, BLIPMultilabelClassifier\n",
    "#pip install --upgrade diffusers transformers nltk accelerate torch_kmeans igraph peft compel torchvision ftfy open_clip_torch einops\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SegmentationConfig:\n",
    "    dir_dataset: str = field(\n",
    "        default=\"/sinergia/ozaydin/datasets/\", metadata={\"help\": \"dir dataset\"}\n",
    "    )\n",
    "    dataset_name: str = field(\n",
    "        default=\"voc2012\", metadata={\"help\": \"for get_dataset\"}\n",
    "    )\n",
    "    split: str = field(\n",
    "        default=\"val\", metadata={\"help\": \"which split to use\"}\n",
    "    )\n",
    "    resolution: int = field(\n",
    "        default=512, metadata={\"help\": \"resolution of the images, e.g, 512, 768, 1024\"}\n",
    "    )\n",
    "    mask_res: int = field(\n",
    "        default=320, metadata={\"help\": \"resolution of the masks, e.g, 64, 320, 512\"}\n",
    "    )\n",
    "    dense_clip_arch: str = field(\n",
    "        default=\"RN50x16\", metadata={\"help\": \"not used in cocostuff\"}\n",
    "    )\n",
    "\n",
    "args = SegmentationConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, categories, palette = get_dataset(\n",
    "    dir_dataset=args.dir_dataset,\n",
    "    dataset_name=args.dataset_name,\n",
    "    split=args.split,\n",
    "    resolution=args.resolution,\n",
    "    mask_res=args.mask_res,\n",
    "    dense_clip_arch=args.dense_clip_arch\n",
    ")\n",
    "label_id_to_cat = categories\n",
    "cat_to_label_id = {v: i for i, v in enumerate(label_id_to_cat)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f9e67c45c1433881906418d30ec755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "# model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "# model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16,\n",
    "    use_safetensors=True, variant=\"fp16\",\n",
    "    # device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# unet_id = \"mhdang/dpo-sd1.5-text2image-v1\"\n",
    "# unet = UNet2DConditionModel.from_pretrained(unet_id, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "# pipe.unet = unet\n",
    "\n",
    "# load_model_weights(pipe, './TexForce/lora_weights/sd15_refl/', 'unet+lora')\n",
    "# load_model_weights(pipe, './TexForce/lora_weights/sd15_texforce/', 'text+lora')\n",
    "# pipe.forward = patch_sd_call(pipe)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "# pipe.enable_freeu(s1=0.9, s2=0.2, b1=1.4, b2=1.6)\n",
    "configure_ldm(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = {}\n",
    "attention_layers_to_use = ATTENTION_LAYERS\n",
    "attention_store = AttentionStore(low_resource=False, no_uncond=True, layer_keys=attention_layers_to_use)\n",
    "attention_store.num_att_layers = len(attention_layers_to_use)\n",
    "handles = register_attention_hooks(pipe.unet, attention_store, attention_layers_to_use, handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_image_and_labels(idx):\n",
    "#     img_path = dataset[idx][\"p_img\"]\n",
    "#     image = Image.open(img_path).convert('RGB').resize((1024, 1024))\n",
    "#     val_gt = dataset[idx][\"gt\"].cpu().numpy()\n",
    "#     val_labels = sorted(np.unique(val_gt))\n",
    "#     val_labels = np.array([l for l in val_labels if l > 0]) - 1\n",
    "#     y_true = np.zeros((20,), dtype=np.int64)\n",
    "#     y_true[val_labels] = 1\n",
    "#     return image, y_true\n",
    "\n",
    "\n",
    "# def get_pred_label_names(y_pred, candidate_labels):\n",
    "#     label_preds = np.array(candidate_labels)[y_pred.astype(bool)]\n",
    "#     assert len(label_preds) > 0\n",
    "#     return label_preds.tolist()\n",
    "\n",
    "\n",
    "# clip_model_id = \"openai/clip-vit-large-patch14\"\n",
    "# clip_classifier = CLIPMultilabelClassifier(clip_model_id, label_id_to_cat[1:])\n",
    "\n",
    "# del clip_classifier.clip_model.text_model\n",
    "\n",
    "# clip_classifier.clip_model.text_model = pipe.text_encoder.text_model\n",
    "# clip_classifier.init_text_embeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mIoU_0 0.583 | : 100%|██████████| 1449/1449 [06:28<00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "def run_exp():\n",
    "    running_score_0 = RunningScore(len(label_id_to_cat))\n",
    "    running_score_1 = RunningScore(len(label_id_to_cat))\n",
    "    running_score_2 = RunningScore(len(label_id_to_cat))\n",
    "\n",
    "    pbar = tqdm(range(len(dataset)))\n",
    "    for idx in pbar:\n",
    "        val_img = dataset[idx][\"img\"].permute(1,2,0)[None,...].numpy()\n",
    "        val_gt = dataset[idx][\"gt\"].unsqueeze(0)\n",
    "        val_labels = sorted(np.unique(val_gt))\n",
    "        val_labels = [l for l in val_labels if l != -1] # don't process label -1 (ignored unlabelled pixels)\n",
    "        val_labels = [label_id_to_cat[c] for c in val_labels]\n",
    "        val_labels = [l for l in val_labels if l != \"background\"] # don't feed \"background\" as text input\n",
    "\n",
    "        # image, y_true = get_image_and_labels(idx)\n",
    "        # y_pred, _ = clip_classifier(image, choice=(8,8), clf_thresh=0.5)\n",
    "        # y_pred = y_pred.cpu().numpy()\n",
    "        # try:\n",
    "        #     val_labels = get_pred_label_names(y_pred, label_id_to_cat[1:])\n",
    "        # except AssertionError:\n",
    "        #     image_indices_failed_clf.append(idx)\n",
    "        #     continue\n",
    "\n",
    "        # text_embeds, concept_ind, _ = get_text_embeddings(pipe.tokenizer, pipe.text_encoder, val_labels, label_id_to_cat)\n",
    "        # text_embeds, concept_ind, _ = get_contextualized_text_embeddings(pipe.tokenizer, pipe.text_encoder, val_labels)\n",
    "        text_embeds, concept_ind, concept_indices, _ = get_txt_embeddings(\n",
    "            pipe.tokenizer, pipe.text_encoder, val_labels, label_id_to_cat, cat_to_label_id,\n",
    "            use_compel=False\n",
    "        )\n",
    "\n",
    "        val_img = val_img.repeat(len(text_embeds), axis=0)\n",
    "        attention_store.reset()\n",
    "\n",
    "        # latents = image2latent(pipe.vae, val_img, normalize=False)\n",
    "        # latents, _, _ = get_noisy_latents(pipe.scheduler, latents)\n",
    "        # pipe.forward(prompt_embeds=text_embeds, latents=latents, guidance_scale=0)\n",
    "\n",
    "        training_step(pipe, text_embeds, val_img, attention_store, no_uncond=True, normalize=False, low_resource=False)\n",
    "\n",
    "        ca, sa = get_attention_maps(\n",
    "            attention_store.get_average_attention(),\n",
    "            batch_size=1,\n",
    "            label_indices=concept_indices,\n",
    "            output_size=64,\n",
    "            average_layers=True,\n",
    "            apply_softmax=True,\n",
    "            softmax_dim=-1,\n",
    "            simple_average=False\n",
    "        )\n",
    "        agg_map = get_agg_map(ca, sa, walk_len=1, beta=1, minmax_norm=True, ca_norm=True)\n",
    "\n",
    "        pred0 = get_random_walk_mask(\n",
    "            agg_map, cat_to_label_id,\n",
    "            [0]+concept_ind, [\"background\"]+val_labels, args.mask_res\n",
    "        ).long()\n",
    "\n",
    "#         pred1 = get_specclust_mask(\n",
    "#             agg_map, sa, cat_to_label_id,\n",
    "#             [0]+concept_ind, [\"background\"]+val_labels,\n",
    "#             output_size=args.mask_res\n",
    "#         ).long()\n",
    "\n",
    "#         pred2 = diffseg(sa, out_res=args.mask_res, refine=True, kl_thresh=0.8)\n",
    "#         agg_map = upscale_attn(agg_map, 320, is_cross=True)[..., [0]+concept_ind]\n",
    "#         pred2 = label_clusters(pred2, agg_map, reshape_logits=False)\n",
    "#         for c in pred2.unique().tolist():        \n",
    "#             pred2[pred2 == c] = cat_to_label_id[([\"background\"]+val_labels)[c]]\n",
    "\n",
    "        running_score_0.update(val_gt.cpu().numpy(), pred0.cpu().numpy())\n",
    "        metrics_0, cls_iou_0 = running_score_0.get_scores()\n",
    "        miou_0 = metrics_0[\"Mean IoU\"]\n",
    "\n",
    "#         running_score_1.update(val_gt.cpu().numpy(), pred1.cpu().numpy())\n",
    "#         metrics_1, cls_iou_1 = running_score_1.get_scores()\n",
    "#         miou_1 = metrics_1[\"Mean IoU\"]\n",
    "\n",
    "#         running_score_2.update(val_gt.cpu().numpy(), pred2.cpu().numpy())\n",
    "#         metrics_2, cls_iou_2 = running_score_2.get_scores()\n",
    "#         miou_2 = metrics_2[\"Mean IoU\"]\n",
    "\n",
    "        # pred2 = DiffusionGraphCut(\n",
    "        #     agg_map.cpu(),\n",
    "        #     sa.cpu(),\n",
    "        #     concept_ind, val_labels, cat_to_label_id\n",
    "        # )(args.mask_res)[None]\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"mIoU_0 {miou_0:.3f} | \"\n",
    "            # f\"mIoU_1 {miou_1:.3f} |\"\n",
    "            # f\"mIoU_2 {miou_2:.3f}\"\n",
    "        )\n",
    "    return [running_score_0]#, running_score_1, running_score_2]\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    rs = run_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pixel Acc': '86.5', 'Mean Acc': '71.0', 'FreqW Acc': '76.9', 'Mean IoU': '58.3'}\n",
      "{'background': '84.9', 'aeroplane': '73.6', 'bicycle': '37.0', 'bird': '62.4', 'boat': '68.8', 'bottle': '48.3', 'bus': '76.6', 'car': '52.9', 'cat': '54.9', 'chair': '31.9', 'cow': '64.5', 'dining table': '39.8', 'dog': '61.8', 'horse': '76.3', 'motorbike': '73.7', 'person': '33.9', 'potted plant': '38.3', 'sheep': '76.5', 'sofa': '54.2', 'train': '69.5', 'tv/monitor': '43.4'}\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rs)):\n",
    "    metrics, cls_iou = rs[i].get_scores() # original results\n",
    "    print({k: f\"{v*100:.1f}\" for k, v in metrics.items()})\n",
    "    print({label_id_to_cat[i]: f\"{v*100:.1f}\" for i, v in cls_iou.items()})\n",
    "    print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train results\n",
    "\n",
    "{'Pixel Acc': '86.5', 'Mean Acc': '69.3', 'FreqW Acc': '76.8', 'Mean IoU': '57.5'}\n",
    "\n",
    "{'background': '85.0', 'aeroplane': '71.8', 'bicycle': '42.7', 'bird': '58.6', 'boat': '66.9', 'bottle': '45.8', 'bus': '73.0', 'car': '56.0', 'cat': '58.3', 'chair': '28.9', 'cow': '64.6', 'dining table': '42.3', 'dog': '64.8', 'horse': '67.3', 'motorbike': '73.6', 'person': '34.3', 'potted plant': '38.1', 'sheep': '74.4', 'sofa': '46.2', 'train': '70.4', 'tv/monitor': '45.0'}\n",
    "\n",
    "Val results\n",
    "\n",
    "{'Pixel Acc': '86.5', 'Mean Acc': '71.0', 'FreqW Acc': '76.9', 'Mean IoU': '58.3'}\n",
    "\n",
    "{'background': '84.9', 'aeroplane': '73.6', 'bicycle': '37.0', 'bird': '62.4', 'boat': '68.8', 'bottle': '48.3', 'bus': '76.6', 'car': '52.9', 'cat': '54.9', 'chair': '31.9', 'cow': '64.5', 'dining table': '39.8', 'dog': '61.8', 'horse': '76.3', 'motorbike': '73.7', 'person': '33.9', 'potted plant': '38.3', 'sheep': '76.5', 'sofa': '54.2', 'train': '69.5', 'tv/monitor': '43.4'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
