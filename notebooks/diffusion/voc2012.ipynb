{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from PIL import Image\n",
    "import os, math, warnings\n",
    "import sys\n",
    "sys.path.append('/scratch/2023-fall-sp-le/langseg')\n",
    "\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets.coco_stuff import coco_stuff_categories\n",
    "from datasets.cityscapes import cat_to_label_id as cityscapes_cats\n",
    "from datasets.utils import get_dataset\n",
    "from utils.metrics import RunningScore\n",
    "from methods.diffusion_patch import *\n",
    "from methods.diffusion import *\n",
    "from methods.diffusion_utils import *\n",
    "from methods.text_embeddings import *\n",
    "from methods.diffusion_seg import *\n",
    "from methods.grabcut import *\n",
    "from methods.pipeline_patch import patch_sd_call, patch_sdxl_call, patch_sd_prepare_latents\n",
    "from methods.multilabel_classifiers import CLIPMultilabelClassifier, BLIPMultilabelClassifier\n",
    "#pip install --upgrade diffusers transformers nltk accelerate torch_kmeans igraph peft compel torchvision ftfy open_clip_torch einops\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SegmentationConfig:\n",
    "    dir_dataset: str = field(\n",
    "        default=\"/sinergia/ozaydin/segment/STEGO-master/data/cocostuff\", metadata={\"help\": \"dir dataset\"}\n",
    "    )\n",
    "    dataset_name: str = field(\n",
    "        default=\"voc2012\", metadata={\"help\": \"for get_dataset\"}\n",
    "    )\n",
    "    split: str = field(\n",
    "        default=\"train\", metadata={\"help\": \"which split to use\"}\n",
    "    )\n",
    "    resolution: int = field(\n",
    "        default=512, metadata={\"help\": \"resolution of the images, e.g, 512, 768, 1024\"}\n",
    "    )\n",
    "    mask_res: int = field(\n",
    "        default=320, metadata={\"help\": \"resolution of the masks, e.g, 64, 320, 512\"}\n",
    "    )\n",
    "    dense_clip_arch: str = field(\n",
    "        default=\"RN50x16\", metadata={\"help\": \"not used in cocostuff\"}\n",
    "    )\n",
    "\n",
    "args = SegmentationConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, categories, palette = get_dataset(\n",
    "    dir_dataset=args.dir_dataset,\n",
    "    dataset_name=args.dataset_name,\n",
    "    split=args.split,\n",
    "    resolution=args.resolution,\n",
    "    mask_res=args.mask_res,\n",
    "    dense_clip_arch=args.dense_clip_arch\n",
    ")\n",
    "label_id_to_cat = categories\n",
    "cat_to_label_id = {v: i for i, v in enumerate(label_id_to_cat)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "# model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "# model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16,\n",
    "    use_safetensors=True, variant=\"fp16\",\n",
    "    # device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# unet_id = \"mhdang/dpo-sd1.5-text2image-v1\"\n",
    "# unet = UNet2DConditionModel.from_pretrained(unet_id, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "# pipe.unet = unet\n",
    "\n",
    "# load_model_weights(pipe, './TexForce/lora_weights/sd15_refl/', 'unet+lora')\n",
    "# load_model_weights(pipe, './TexForce/lora_weights/sd15_texforce/', 'text+lora')\n",
    "# pipe.forward = patch_sd_call(pipe)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "# pipe.enable_freeu(s1=0.9, s2=0.2, b1=1.4, b2=1.6)\n",
    "configure_ldm(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = {}\n",
    "attention_layers_to_use = ATTENTION_LAYERS\n",
    "attention_store = AttentionStore(low_resource=False, no_uncond=True, layer_keys=attention_layers_to_use)\n",
    "attention_store.num_att_layers = len(attention_layers_to_use)\n",
    "handles = register_attention_hooks(pipe.unet, attention_store, attention_layers_to_use, handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_image_and_labels(idx):\n",
    "#     img_path = dataset[idx][\"p_img\"]\n",
    "#     image = Image.open(img_path).convert('RGB').resize((1024, 1024))\n",
    "#     val_gt = dataset[idx][\"gt\"].cpu().numpy()\n",
    "#     val_labels = sorted(np.unique(val_gt))\n",
    "#     val_labels = np.array([l for l in val_labels if l > 0]) - 1\n",
    "#     y_true = np.zeros((20,), dtype=np.int64)\n",
    "#     y_true[val_labels] = 1\n",
    "#     return image, y_true\n",
    "\n",
    "\n",
    "# def get_pred_label_names(y_pred, candidate_labels):\n",
    "#     label_preds = np.array(candidate_labels)[y_pred.astype(bool)]\n",
    "#     assert len(label_preds) > 0\n",
    "#     return label_preds.tolist()\n",
    "\n",
    "\n",
    "# clip_model_id = \"openai/clip-vit-large-patch14\"\n",
    "# clip_classifier = CLIPMultilabelClassifier(clip_model_id, label_id_to_cat[1:])\n",
    "\n",
    "# del clip_classifier.clip_model.text_model\n",
    "\n",
    "# clip_classifier.clip_model.text_model = pipe.text_encoder.text_model\n",
    "# clip_classifier.init_text_embeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_score_0 = RunningScore(len(label_id_to_cat))\n",
    "running_score_1 = RunningScore(len(label_id_to_cat))\n",
    "running_score_2 = RunningScore(len(label_id_to_cat))\n",
    "\n",
    "pbar = tqdm(range(len(dataset)))\n",
    "for idx in pbar:\n",
    "    val_img = dataset[idx][\"img\"].permute(1,2,0)[None,...].numpy()\n",
    "    val_gt = dataset[idx][\"gt\"].unsqueeze(0)\n",
    "    val_labels = sorted(np.unique(val_gt))\n",
    "    val_labels = [l for l in val_labels if l != -1] # don't process label -1 (ignored unlabelled pixels)\n",
    "    val_labels = [label_id_to_cat[c] for c in val_labels]\n",
    "    val_labels = [l for l in val_labels if l != \"background\"] # don't feed \"background\" as text input\n",
    "\n",
    "    # image, y_true = get_image_and_labels(idx)\n",
    "    # y_pred, _ = clip_classifier(image, choice=(8,8), clf_thresh=0.5)\n",
    "    # y_pred = y_pred.cpu().numpy()\n",
    "    # try:\n",
    "    #     val_labels = get_pred_label_names(y_pred, label_id_to_cat[1:])\n",
    "    # except AssertionError:\n",
    "    #     image_indices_failed_clf.append(idx)\n",
    "    #     continue\n",
    "\n",
    "    # text_embeds, concept_ind, _ = get_text_embeddings(pipe.tokenizer, pipe.text_encoder, val_labels, label_id_to_cat)\n",
    "    # text_embeds, concept_ind, _ = get_contextualized_text_embeddings(pipe.tokenizer, pipe.text_encoder, val_labels)\n",
    "    text_embeds, concept_ind, concept_indices, _ = get_txt_embeddings(\n",
    "        pipe.tokenizer, pipe.text_encoder, val_labels, label_id_to_cat, cat_to_label_id,\n",
    "        use_compel=True\n",
    "    )\n",
    "\n",
    "    val_img = val_img.repeat(len(text_embeds), axis=0)\n",
    "    attention_store.reset()\n",
    "\n",
    "    # latents = image2latent(pipe.vae, val_img, normalize=False)\n",
    "    # latents, _, _ = get_noisy_latents(pipe.scheduler, latents)\n",
    "    # pipe.forward(prompt_embeds=text_embeds, latents=latents, guidance_scale=0)\n",
    "    \n",
    "    training_step(pipe, text_embeds, val_img, attention_store, no_uncond=True, normalize=False, low_resource=False)\n",
    "\n",
    "    ca, sa = get_attention_maps(\n",
    "        attention_store.get_average_attention(),\n",
    "        batch_size=1,\n",
    "        label_indices=concept_indices,\n",
    "        output_size=64,\n",
    "        average_layers=True,\n",
    "        apply_softmax=True,\n",
    "        softmax_dim=-1,\n",
    "        simple_average=False\n",
    "    )\n",
    "    agg_map = get_agg_map(ca, sa, walk_len=1, beta=1)\n",
    "\n",
    "    pred_mask_0 = get_random_walk_mask(\n",
    "        agg_map, cat_to_label_id,\n",
    "        [0]+concept_ind, [\"background\"]+val_labels, args.mask_res\n",
    "    )\n",
    "    pred_mask_0 = pred_mask_0.long().cpu().numpy()\n",
    "    running_score_0.update(val_gt.cpu().numpy(), pred_mask_0)\n",
    "    metrics_0, cls_iou_0 = running_score_0.get_scores()\n",
    "    miou_0 = metrics_0[\"Mean IoU\"]\n",
    "\n",
    "    # pred_mask_1 = get_specclust_mask(\n",
    "    #     agg_map, sa, cat_to_label_id,\n",
    "    #     [0]+concept_ind, [\"background\"]+val_labels,\n",
    "    #     output_size=args.mask_res, bg_thresh=0.32\n",
    "    # )\n",
    "    # bg_filter = F.interpolate(\n",
    "    #     agg_map, size=args.mask_res, mode=\"bilinear\", align_corners=False\n",
    "    # )\n",
    "    # pred_mask_1 = torch.where(bg_filter[:, 0] < bg_filter[:, 0].mean(), pred_mask_1, 0)\n",
    "    # running_score_1.update(val_gt.cpu().numpy(), pred_mask_1.long().cpu().numpy())\n",
    "    # metrics_1, cls_iou_1 = running_score_1.get_scores()\n",
    "    # miou_1 = metrics_1[\"Mean IoU\"]\n",
    "    \n",
    "    # pred_mask_2 = DiffusionGraphCut(\n",
    "    #     agg_map.cpu(),\n",
    "    #     sa.cpu(),\n",
    "    #     concept_ind, val_labels, cat_to_label_id\n",
    "    # )(args.mask_res)[None]\n",
    "    # pred_mask_2 = pred_mask_2.cpu().numpy()\n",
    "    # running_score_2.update(val_gt.cpu().numpy(), pred_mask_2)\n",
    "    # metrics_2, cls_iou_2 = running_score_2.get_scores()\n",
    "    # miou_2 = metrics_2[\"Mean IoU\"]\n",
    "\n",
    "    pbar.set_description(\n",
    "        f\"mIoU_0 {miou_0:.3f} | \"\n",
    "        # f\"mIoU_1 {miou_1:.3f} |\"\n",
    "        # f\"mIoU_2 {miou_2:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print({k: f\"{v*100:.1f}\" for k, v in metrics_0.items()})\n",
    "print({label_id_to_cat[i]: f\"{v*100:.1f}\" for i, v in cls_iou_0.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print({k: f\"{v*100:.1f}\" for k, v in metrics_1.items()})\n",
    "# print({label_id_to_cat[i]: f\"{v*100:.1f}\" for i, v in cls_iou_1.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print({k: f\"{v*100:.1f}\" for k, v in metrics_2.items()})\n",
    "# print({label_id_to_cat[i]: f\"{v*100:.1f}\" for i, v in cls_iou_2.items()})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
